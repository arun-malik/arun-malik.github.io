<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Transformers (Intro) ¬∑ Observation Odyssey</title>
    <meta name="description" content="A transcript-driven, interactive walkthrough of the Transformer architecture: why it replaced RNNs/LSTMs and how attention lets tokens communicate." />
    <link rel="icon" href="/_shared/images/logos/OO.svg" type="image/svg+xml" />
    <link rel="icon" href="/_shared/images/logos/OO.png" type="image/png" />
    <link rel="apple-touch-icon" href="/_shared/images/logos/OO.png" />
    <link rel="stylesheet" href="/_shared/viki-styles.css" />
  </head>
  <body>
    <button class="theme-toggle" id="themeToggle" title="Toggle theme">
      <span id="themeIcon">üåô</span>
    </button>

    <div class="header-wrapper" id="header-placeholder"></div>

    <div class="horizontal-scroll-container ai-page-scroll">
      <main class="scroll-content">
        <section class="ai-lesson ai-page" id="launch">
          <div class="card-icon">üß†</div>
          <h1 class="card-title" style="margin: 0 0 8px 0; font-size: 1.6rem;">How one paper reshaped AI: Transformers</h1>
          <p class="section-text" style="margin-top: 0; color: var(--text-secondary);">
            A transcript-driven, interactive walkthrough of the Transformer architecture: why it replaced RNNs/LSTMs and how attention lets tokens communicate.
          </p>

          <div class="lesson-quote">
            <div class="lesson-quote-title">If you remember one thing</div>
            <div class="lesson-quote-text">A transformer is a network that lets its inputs talk to each other. It‚Äôs not magic ‚Äî it‚Äôs communication.</div>
          </div>

          <div class="diagram-tour" data-diagram-tour>
            <div class="diagram-tour-header">
              <div class="diagram-tour-heading">
                <div class="diagram-tour-kicker">Interactive walkthrough</div>
                <div class="diagram-tour-title" data-tour-title>Start here</div>
              </div>
              <div class="diagram-tour-controls">
                <button type="button" class="diagram-tour-btn" data-tour-prev>Prev</button>
                <span class="diagram-tour-progress" data-tour-progress></span>
                <button type="button" class="diagram-tour-btn" data-tour-next>Next</button>
              </div>
            </div>
            <div class="diagram-tour-body" data-tour-body></div>
            <div class="diagram-tour-hint">Tip: click any box to jump to its explanation.</div>
          </div>

          <div class="lesson-toc" aria-label="Lesson sections">
            <a class="lesson-toc-link" href="#ml">ML mapping</a>
            <a class="lesson-toc-link" href="#sequences">Why sequences are hard</a>
            <a class="lesson-toc-link" href="#transformer">The transformer idea</a>
            <a class="lesson-toc-link" href="#encoder-decoder">Encoder / decoder</a>
            <a class="lesson-toc-link" href="#example">Worked example</a>
            <a class="lesson-toc-link" href="#flow">Data flow</a>
            <a class="lesson-toc-link" href="#qkv">Attention (Q,K,V)</a>
            <a class="lesson-toc-link" href="#training">How it learns</a>
            <a class="lesson-toc-link" href="#variants">Variants</a>
            <a class="lesson-toc-link" href="#generalize">Beyond text</a>
          </div>

          <div class="card-section">
            <h4 class="section-label" id="ml">ML mapping</h4>
            <p class="section-text" style="margin-bottom: 10px;">
              The goal of machine learning is to learn a mapping from inputs to outputs. Neural networks learn this by stacking layers that transform representations.
            </p>
            <div class="diagram" role="img" aria-label="High level diagram: inputs to outputs mapping; sequential problems with RNNs; transformer uses attention for all-to-all communication; parallel training and long-range context.">
              <div class="diagram-row">
                <div class="diagram-box" data-node="ml-goal">
                  <div class="diagram-title">Machine Learning Goal</div>
                  <div class="diagram-sub">Learn a mapping: Inputs ‚Üí Outputs</div>
                </div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="neural-network">
                  <div class="diagram-title">Neural Network</div>
                  <div class="diagram-sub">Stacked layers transform representations</div>
                </div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="task-output">
                  <div class="diagram-title">Task Output</div>
                  <div class="diagram-sub">Price, label, next token, ‚Ä¶</div>
                </div>
              </div>

              <div class="diagram-row" style="margin-top: 14px;">
                <div class="diagram-box" data-node="sequential-data">
                  <div class="diagram-title">Sequential Data</div>
                  <div class="diagram-sub">Words / tokens need context</div>
                </div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="rnn-lstm">
                  <div class="diagram-title">RNN / LSTM</div>
                  <div class="diagram-sub">One token at a time (slow)</div>
                </div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="rnn-problems">
                  <div class="diagram-title">Problems</div>
                  <div class="diagram-sub">No parallelism + weak long-range memory</div>
                </div>
              </div>

              <div class="diagram-row" style="margin-top: 14px;">
                <div class="diagram-box" data-node="transformer-2017">
                  <div class="diagram-title">Transformer (2017)</div>
                  <div class="diagram-sub">Adds an attention ‚Äúcommunication layer‚Äù</div>
                </div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="all-to-all">
                  <div class="diagram-title">All-to-all context</div>
                  <div class="diagram-sub">Every token can look at every other token</div>
                </div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="why-won">
                  <div class="diagram-title">Why it won</div>
                  <div class="diagram-sub">Parallel training + long dependencies</div>
                </div>
              </div>
            </div>
          </div>

          <div class="card-section">
            <h4 class="section-label" id="sequences">Why sequences are hard</h4>
            <p class="section-text">
              For language, each token depends on context. If tokens are processed independently, meaning gets lost. RNNs/LSTMs tried to carry context with a moving memory, but training stayed sequential and long-range information faded.
            </p>
          </div>

          <div class="card-section">
            <h4 class="section-label" id="transformer">The transformer idea</h4>
            <p class="section-text">
              Transformers keep the ‚Äústacked layers‚Äù idea, but add attention ‚Äî a communication layer where every token can look at every other token and decide what matters.
            </p>
          </div>

          <div class="card-section">
            <h4 class="section-label">Transformer Block (repeated N times)</h4>
            <div class="diagram" role="img" aria-label="Transformer block diagram: attention mixes information across tokens; MLP refines per-token; residual and layer norm stabilize.">
              <div class="diagram-row">
                <div class="diagram-box" data-node="block-attention">
                  <div class="diagram-title">Attention</div>
                  <div class="diagram-sub">Tokens communicate (mix information)</div>
                </div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="block-mlp">
                  <div class="diagram-title">MLP / Feed-Forward</div>
                  <div class="diagram-sub">Each token refines privately</div>
                </div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="block-stability">
                  <div class="diagram-title">Stable Training</div>
                  <div class="diagram-sub">Residual connections + LayerNorm</div>
                </div>
              </div>
            </div>
          </div>

          <div class="card-section">
            <h4 class="section-label" id="encoder-decoder">Encoder / decoder (classic Transformer)</h4>
            <p class="section-text" style="margin-bottom: 10px;">
              The original 2017 Transformer uses an encoder-decoder. The encoder builds representations of the input. The decoder generates output tokens, using both its own history and the encoder‚Äôs output.
            </p>
            <div class="diagram" role="img" aria-label="Encoder decoder diagram: encoder stack produces memory; decoder stack uses masked self-attention, cross-attention over encoder memory, and MLP, then outputs tokens.">
              <div class="diagram-row">
                <div class="diagram-box" data-node="enc-input"><div class="diagram-title">Input tokens</div><div class="diagram-sub">Source sequence</div></div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="encoder"><div class="diagram-title">Encoder stack</div><div class="diagram-sub">Self-attention + MLP √ó N</div></div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="memory"><div class="diagram-title">Encoder memory</div><div class="diagram-sub">Context for decoding</div></div>
              </div>
              <div class="diagram-row" style="margin-top: 14px;">
                <div class="diagram-box" data-node="dec-input"><div class="diagram-title">Previous outputs</div><div class="diagram-sub">Shifted right tokens</div></div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="decoder"><div class="diagram-title">Decoder stack</div><div class="diagram-sub">Masked self-attn + cross-attn + MLP √ó N</div></div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="dec-output"><div class="diagram-title">Next token</div><div class="diagram-sub">Generated output</div></div>
              </div>
            </div>

            <details class="lesson-details">
              <summary>Where do BERT and GPT fit?</summary>
              <div class="section-text">
                Encoder-only models (BERT-style) are great at understanding and classification. Decoder-only models (GPT-style) are great at generation.
              </div>
            </details>
          </div>

          <div class="card-section" id="example">
            <h4 class="section-label">Worked example: ‚Äúit‚Äù needs context</h4>
            <p class="section-text" style="margin-bottom: 10px;">
              In attention, a token can look at all other tokens and decide what‚Äôs relevant. Try a toy example (not a real model):
            </p>

            <div class="attention-demo" data-attention-demo data-sentence="Jake learned AI even though it was difficult .">
              <div class="attention-demo-row">
                <div class="attention-demo-label">Sentence</div>
                <div class="token-strip" data-token-strip aria-label="Click a token to focus it"></div>
              </div>

              <div class="attention-demo-row" style="margin-top: 12px;">
                <div class="attention-demo-label">Focus token</div>
                <div class="attention-demo-focus" data-focus-label></div>
              </div>

              <div class="attention-demo-row" style="margin-top: 10px;">
                <div class="attention-demo-label">Attention weights</div>
                <div class="attn-weights" data-attn-weights></div>
              </div>

              <div class="section-text" style="margin-top: 10px;">
                Click different tokens to see how ‚Äúfocus‚Äù changes. In real transformers, these weights are learned during training.
              </div>
            </div>
          </div>

          <div class="card-section">
            <h4 class="section-label" id="flow">Data flow (text ‚Üí representations)</h4>
            <p class="section-text" style="margin-bottom: 10px;">
              Text becomes tokens, tokens become vectors (embeddings), and we add positional information so the model knows order.
            </p>
            <div class="diagram" role="img" aria-label="Data flow diagram: tokenizer to tokens, embeddings, positional info, repeated blocks, final contextual vectors for tasks like generation or classification.">
              <div class="diagram-row">
                <div class="diagram-box" data-node="tokenizer"><div class="diagram-title">Tokenizer</div><div class="diagram-sub">Text ‚Üí tokens</div></div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="embedding"><div class="diagram-title">Embedding</div><div class="diagram-sub">Tokens ‚Üí vectors</div></div>
                <div class="diagram-arrow">+</div>
                <div class="diagram-box" data-node="positional"><div class="diagram-title">Positional Info</div><div class="diagram-sub">Adds order</div></div>
              </div>
              <div class="diagram-row" style="margin-top: 14px;">
                <div class="diagram-box" data-node="stacked-blocks"><div class="diagram-title">Stacked Blocks</div><div class="diagram-sub">Attention + MLP repeated</div></div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="contextual-vectors"><div class="diagram-title">Contextual Vectors</div><div class="diagram-sub">One vector per token</div></div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="task-head"><div class="diagram-title">Task Head</div><div class="diagram-sub">Next token / classifier / etc.</div></div>
              </div>
            </div>

            <details class="lesson-details">
              <summary>Why positional info matters</summary>
              <div class="section-text">
                Attention doesn‚Äôt know order by default. Positional signals make ‚ÄúJake learned AI‚Äù different from ‚ÄúAI learned Jake‚Äù.
              </div>
            </details>
          </div>

          <div class="card-section analogy-box">
            <h4 class="section-label" id="qkv">Zoom-in: Attention (Q, K, V)</h4>
            <p class="section-text" style="margin: 0 0 10px 0;">
              Each token creates a Query (what I‚Äôm looking for), a Key (what I match on), and a Value (what I share).
            </p>
            <div class="diagram" role="img" aria-label="Attention math diagram: Q queries compare with K keys, softmax becomes weights, weighted sum over V values produces new representation; done in parallel with matrices.">
              <div class="diagram-row">
                <div class="diagram-box" data-node="q"><div class="diagram-title">Query (Q)</div><div class="diagram-sub">What am I looking for?</div></div>
                <div class="diagram-arrow">√ó</div>
                <div class="diagram-box" data-node="k"><div class="diagram-title">Keys (K)</div><div class="diagram-sub">What do others have?</div></div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="scores"><div class="diagram-title">Scores</div><div class="diagram-sub">Similarity via dot products</div></div>
              </div>
              <div class="diagram-row" style="margin-top: 14px;">
                <div class="diagram-box" data-node="softmax"><div class="diagram-title">Softmax</div><div class="diagram-sub">Normalize ‚Üí attention weights</div></div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="v"><div class="diagram-title">Values (V)</div><div class="diagram-sub">Content to share</div></div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="weighted-sum"><div class="diagram-title">Weighted Sum</div><div class="diagram-sub">New token representation</div></div>
              </div>
              <p class="section-text" style="margin: 12px 0 0 0; color: var(--text-secondary);">
                Computed in parallel by stacking Q, K, V into matrices ‚Äî that‚Äôs why training is fast.
              </p>
            </div>

            <details class="lesson-details">
              <summary>Attention formula (intuition)</summary>
              <div class="section-text">
                $\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(QK^\top / \sqrt{d_k})\,V$ ‚Äî match queries with keys, then blend values using the resulting weights.
              </div>
            </details>
          </div>

          <div class="card-section" id="training">
            <h4 class="section-label">How it learns (why the weights mean something)</h4>
            <p class="section-text" style="margin-bottom: 10px;">
              At the start, parameters are random, so attention is meaningless. During training, the model learns patterns (like pronouns looking to relevant nouns).
            </p>
            <div class="diagram" role="img" aria-label="Training progression diagram: random attention becomes meaningful patterns after optimization.">
              <div class="diagram-row">
                <div class="diagram-box" data-node="random-init"><div class="diagram-title">Start: random</div><div class="diagram-sub">Queries/keys/values are noise</div></div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="training-loop"><div class="diagram-title">Training</div><div class="diagram-sub">Optimize to reduce loss</div></div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box" data-node="learned-patterns"><div class="diagram-title">After learning</div><div class="diagram-sub">Attention captures structure</div></div>
              </div>
            </div>
          </div>

          <div class="card-section" id="variants">
            <h4 class="section-label">Variants (masked, multi-head, cross)</h4>
            <p class="section-text" style="margin-bottom: 10px;">
              Variations enforce causality (masked), combine multiple attention ‚Äúviews‚Äù (multi-head), or mix information from another sequence (cross-attention).
            </p>
            <div class="diagram" role="img" aria-label="Attention variants diagram: masked attention for causality, multi-head for multiple subspaces, cross-attention to attend to encoder memory.">
              <div class="diagram-row">
                <div class="diagram-box" data-node="masked"><div class="diagram-title">Masked attention</div><div class="diagram-sub">Can‚Äôt look ahead</div></div>
                <div class="diagram-arrow">¬∑</div>
                <div class="diagram-box" data-node="multihead"><div class="diagram-title">Multi-head</div><div class="diagram-sub">Multiple heads</div></div>
                <div class="diagram-arrow">¬∑</div>
                <div class="diagram-box" data-node="cross"><div class="diagram-title">Cross-attention</div><div class="diagram-sub">Attend to another sequence</div></div>
              </div>
            </div>
          </div>

          <div class="card-section">
            <h4 class="section-label" id="generalize">Why it generalized beyond text</h4>
            <div class="diagram" role="img" aria-label="Generalization diagram: transformers apply to any data as sequences where elements interact: text, images, audio, code.">
              <div class="diagram-row">
                <div class="diagram-box"><div class="diagram-title">Text</div><div class="diagram-sub">Tokens</div></div>
                <div class="diagram-arrow">¬∑</div>
                <div class="diagram-box"><div class="diagram-title">Images</div><div class="diagram-sub">Patches</div></div>
                <div class="diagram-arrow">¬∑</div>
                <div class="diagram-box"><div class="diagram-title">Audio</div><div class="diagram-sub">Frames</div></div>
                <div class="diagram-arrow">¬∑</div>
                <div class="diagram-box"><div class="diagram-title">Code</div><div class="diagram-sub">Tokens</div></div>
              </div>
              <p class="section-text" style="margin: 12px 0 0 0; color: var(--text-secondary);">
                If you can represent data as a sequence of elements that should ‚Äútalk‚Äù to each other, transformers shine.
              </p>
            </div>
          </div>

          <div class="card-section">
            <a href="../../../" class="nav-link" style="display: inline-block; border-bottom-color: transparent;">‚Üê Back to AI</a>
          </div>
        </section>
      </main>
    </div>

    <div class="footer-wrapper" id="footer-placeholder"></div>

    <script src="/_shared/components.js"></script>
    <script src="/_shared/scroll-nav.js"></script>
    <script src="/_shared/theme.js"></script>
    <script src="/_shared/interactive-diagram.js"></script>
    <script src="/_shared/attention-demo.js"></script>
  </body>
</html>
